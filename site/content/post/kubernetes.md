+++
author = "James"
date = "2017-05-05T19:12:32+01:00"
menu = "main"
tags = ["Google","Ruby","Kubernetes","Coding"]
title = "Kubernetes Shenanigans"

+++

**Yesterday** I attended Google Next in London. As you might expect from an event with a ridiculously large corporate sponsor it was somewhat heavyhanded on the advertising (though, Google, so...), but the food was free and the talks were interesting so it had that going for it.

It was fairly easy to tell what they were pushing hardest, behind the 'Open Cloud, Open Development' message - machine learning, containerisation and their shiny new-to-the-public distributed database 'Spanner'. The latter looks like a phenomenal upgrade from MySQL, but unfortunately there's no ActiveRecord plugin for it, so it's currently useless for my main project at work. I have hopes, though; the Ruby libraries for most of the Cloud Platform are well supported, so there's clearly a known market.
Inapplicable shinies notwithstanding I thought it would be interesting, having been talked through a number of these tools, to have a shot at one of the more complicated ones: Kubernetes. For some reason this is often spelled 'K8s', which didn't seem at all obvious until I heard a presenter pronounce it exactly as that implies.

This blog is, as mentioned in previous posts, generated by [Hugo](https://gohugo.io). As such it's pretty portable; a collection of static files and a single executable, which happens to be its own server. That's an excellent candidate for a first use of K8s, requiring only a little configuration to get up and running.

I can hear those of you who've tried this before laughing.

The Kubernetes documentation is extensive, excellent, and fairly useless in the way that only a vertical learning curve can be. Every component can be queried, but you have to know exactly how; most tutorials assume you know at least one thing that isn't actually obvious.

In this case I turned out to need a container for the site (that was straightforward enough), one for an Nginx reverse proxy so I could have SSL and potentially load balancing, a service to expose the former to the latter, a service to expose the latter to the internet, and an actual Google Cloud L4 load balancer so I had something to point at.

I'm going to cover those in that order, because it's easier to explain how they chain that way. For the site, I used the below configuration file. Being able to `kubectl apply` a deployment is really handy; Deployments are declarative, and therefore idempotent, so I can have faith that if I make a small change that's the only thing that will be touched.
I've used the `extensions/v1beta1` API version, because Deployments aren't available in `v1`.

Metadata have both human and machine meanings. The `name` field appears to be just for differentiation, but the `labels` list is very handy - services will use their `selector` field to search for containers with a given label or labels, which means this determines which pods are treated as similar for the purposes of being a backend to a given service.

Following that is the spec for what will actually be deployed. The number of replicas is pretty obvious, although what might not be is that each requests (even if it's not using them) a certain amount of resources, which can mean that if you leave the default (i.e. don't provide a `request` and a `limit`) you have no space for more pods even though your apparent CPU load on the Node is minimal.
The image is pulled from Google's Container Registry; this is quite handy as I can have it automatically build the next container when I push to Github. That's very important when you're lazy like I am, although I do still have to tell it to update the pods once it's built.
I've exposed port 1313; this is because the Hugo server inside is serving on localhost:1313; binding that port to the outside of the container means it can be directly accessed by other services.


```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: jamieduerden-me
  labels: 
    app: jamieduerden-blog
spec:
  replicas: 2
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: jamieduerden-blog
    spec:
      containers:
        - image: gcr.io/jamieduerden-me/hugo-public:a8db378ef8d6127ae62e39ae1fb773c137da06c2
          imagePullPolicy: Always
          name: jamieduerden-me
          ports:
          - containerPort: 1313
          readinessProbe:
            httpGet:
              path: /
              port: 1313
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 2
            failureThreshold: 5
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "150m"
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
```

We then need a service which will abstract away the problem of connecting to both of these pods without having to know their IP addresses or care whether they are recreated. This looks like the below; a pretty straightforward declaration.
It has a name, it looks for the selector `app=jamieduerden-blog`, and it connects its own port 1313 to the ports 1313 of the pods. This also creates some handy hooks, which I'll cover below.

```
---
  kind: Service
  apiVersion: v1
  metadata:
    name: jamieduerden-blog
  spec:
    type: NodePort
    selector:
        app: jamieduerden-blog
    ports:
      - protocol: TCP
        port: 1313
        targetPort: 1313
        name: blog

```

The one for Nginx looks pretty similar, except I'm using a standard image and there's only one of them.

The important thing here is the `env:` list. Kubernetes produces, automatically, some helpful environment variables of the form `{SERVICE_NAME}_SERVICE_(HOST | PORT_{PORT_NAME})` when you set up a service which points to a collection of pods.
We can look for these for this case specifically (`JAMIEDUERDEN_BLOG...`) and assign them to more general environment variables inside the Nginx pod. These are used by the Docker image I'm pulling for this proxy to create an Nginx config on the fly, which is very cool and a trick I will have to steal and extend at some point.
SSL is disabled for the moment while I sort out Secrets, which is the next stage, and then make the necessary DNS changes. I'm hoping I can still use LetsEncrypt to get my SSL certificates. If anyone can persuade someone at Google to build that in to the GC Load Balancer, that would be most pleasing.

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ssl-proxy
  labels:
    name: nginx
    role: ssl-proxy
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      name: nginx-ssl-proxy
      labels:
        name: nginx
        role: ssl-proxy
    spec:
      containers:
      - name: nginx-ssl-proxy
        image: gcr.io/cloud-solutions-images/nginx-ssl-proxy:master-9979ee3
        command:
        - /bin/bash
        - /usr/bin/start.sh
        env:
        - name: "SERVICE_HOST_ENV_NAME"
          value: "JAMIEDUERDEN_BLOG_SERVICE_HOST"
        - name: "SERVICE_PORT_ENV_NAME"
          value: "JAMIEDUERDEN_BLOG_SERVICE_PORT_BLOG"
        - name: ENABLE_SSL
          value: 'false'
        - name: ENABLE_BASIC_AUTH
          value: 'false'
        ports:
        - name: ssl-proxy-http
          containerPort: 80
        - name: ssl-proxy-https
          containerPort: 443
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30

```

The final piece of the puzzle, at least for the non-SSL, uncomplicated but functional level I've got to, is a service to bind the Nginx pod to the outside world.

Giving it the `type: LoadBalancer` declaration has a very useful side effect; it creates a dedicated L4 load balancer in GC and prepopulates the endpoints and forwarding rule to send the declared ports to the right place.
When it doesn't work you don't get many hints why, but once it does it's a one-command link between the internet and the pods.

```
---
kind: Service
apiVersion: v1
metadata:
  name: nginx-ssl-proxy
  labels:
    name: nginx
    role: ssl-proxy
spec:
  ports:
  - name: https
    port: 443
    targetPort: ssl-proxy-https
    protocol: TCP
  - name: http
    port: 80
    targetPort: ssl-proxy-http
    protocol: TCP
  selector:
    name: nginx
    role: ssl-proxy
  type: LoadBalancer
```

To my great relief it turns out that once written, you can dump all these into one folder and execute the whole lot at once with `kubectl apply -f $FOLDERNAME`. This makes taking the entire collection down and putting it up again is the work of moments, rather than hours.

All in all, it was a fairly successful experiment - I expect you're reading this served from the new infrastructure - but it took longer to read the documentation and tutorials than it really should have.

